{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRO TO ARTIFICIAL INTELLIGENCE\n",
    "# FINAL PROJECT\n",
    "## NEURAL NETWORK FOR DIGIT RECOGNITION TRAINED FROM MNIST DATASET\n",
    "\n",
    "### By Carlo D. Pastoral - N26087048\n",
    "[YouTube Video Presentation ](https://youtu.be/AUh1xQiWESk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "To build my neuralnetwork, I used the following Packages:\n",
    "\n",
    "* Numpy - Used for manipulating and computing large multi-dimensional arrays and matrices. \n",
    "* Tensorflow - A library for symbolic math used for neural networks.\n",
    "* Terflow Datasets - An API provided by Tensorflow for the ease of accessing a wide range of open source and free datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "* Using the **load** function of the **tensorflow_datasets**, I downloaded the **MNIST** dataset directly from tensorflow. \n",
    "* This function also allows me to simultaeneously split the dataset into train and test sets.\n",
    "* Aside from splitting the data, I am also able to shuffle it.\n",
    "* **tfds.load** function, also ships the dataset info that is very useful in knowing the content of the dataset, its reference, and most importantly the dataset features that can be used for data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tfds.core.DatasetInfo(\n    name='mnist',\n    version=3.0.1,\n    description='The MNIST database of handwritten digits.',\n    homepage='http://yann.lecun.com/exdb/mnist/',\n    features=FeaturesDict({\n        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n    }),\n    total_num_examples=70000,\n    splits={\n        'test': 10000,\n        'train': 60000,\n    },\n    supervised_keys=('image', 'label'),\n    citation=\"\"\"@article{lecun2010mnist,\n      title={MNIST handwritten digit database},\n      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n      volume={2},\n      year={2010}\n    }\"\"\",\n    redistribution_info=,\n)"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "* In this part I applied a normalization for each image in the dataset. \n",
    "* Normalizing is mainly used for making all the data in the same scale. \n",
    "* Normalizing the data also helps reducing the computing requirements.\n",
    "\n",
    "* **Caching the dataset** - the **cache** fucntion of tfds is useful in loading the dataset during training as it is making it faster and significantly reducing memory consumption. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model\n",
    "\n",
    "* Here I designed a very simple FCNN that has only 3 layers. Input, hidden layer, and Output.\n",
    "\n",
    "* **Flatten** - as an input layer transforms the 2D image into a vector. \n",
    "* **Dense**  - As a hidden layer that produces 128 feature maps activated by relu. \n",
    "* **Dense**  - As a output layer that produces 62 probabilities activated by softmax. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/5\n469/469 [==============================] - 4s 9ms/step - loss: 0.4443 - accuracy: 0.8831 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 2/5\n469/469 [==============================] - 2s 5ms/step - loss: 0.1952 - accuracy: 0.9461 - val_loss: 0.1540 - val_accuracy: 0.9552\nEpoch 3/5\n469/469 [==============================] - 2s 5ms/step - loss: 0.1422 - accuracy: 0.9597 - val_loss: 0.1270 - val_accuracy: 0.9623\nEpoch 4/5\n469/469 [==============================] - 2s 5ms/step - loss: 0.1117 - accuracy: 0.9683 - val_loss: 0.1113 - val_accuracy: 0.9663\nEpoch 5/5\n469/469 [==============================] - 2s 4ms/step - loss: 0.0911 - accuracy: 0.9749 - val_loss: 0.1004 - val_accuracy: 0.9689\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x13e07894588>"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Dense(128,activation='relu'),\n",
    "  tf.keras.layers.Dense(62, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=5,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('MNIST_.31%_Carlo_Pastoral.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "* For a very simple FCNN, it gives me a ~96% accuracy. \n",
    "* To further test the model, here I reloaded the mnist testset without shuffling. \n",
    "* I used 10 images from selected indexes to made a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "79/79 - 0s - loss: 0.1004 - accuracy: 0.9689\n\nTest accuracy: 0.9689\n[0 7 9 9 9 6 5 8 8 6]\n[0 7 9 9 9 6 5 8 8 6]\n"
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(ds_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "image, label = tfds.as_numpy(tfds.load(\n",
    "    'mnist',\n",
    "    split='test', \n",
    "    batch_size=-1, \n",
    "    as_supervised=True,\n",
    "))\n",
    "\n",
    "predictions = model.predict(image) \n",
    "print(np.argmax(predictions[50:60],axis=1))\n",
    "print(label[50:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitdextrconda9c44973e2453494f965afd3c68827a3f",
   "display_name": "Python 3.6.10 64-bit ('dextr': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}